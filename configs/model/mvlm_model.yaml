_target_: src.models.maskvlm_module.MaskVLMModule

# contrastive_loss
itc_loss:
  _target_: src.models.criterion.ITCLoss
  lamda: 0.5
  temperature: 0.07

# itc_loss: # use this if you want to use the contrasitve loss of ConVIRT
#   _target_: src.models.criterion.ConVIRTContrastiveCriterion
#   temperature: 0.1
#   lamda: 0.75

# loss weights
loss_weight_itc: 1.0
loss_weight_itm: 1.0
loss_weight_mlm: 0.5
loss_weight_mim: 0.5

image_patch_size: 16

# configure optimizer
weight_decay: 0.05
lr_encoder: 1e-5
lr_cross: 3e-4
lr_other: 3e-4
warmup_epochs: 5
max_epochs: ${trainer.max_epochs}

net:
  _target_: src.models.maskvlm_model.MaskVLM
  image_model:
    _target_: src.models.image_encoder.MaskVLMImageEncoder
    name: google/vit-base-patch16-224
  text_model:
    _target_: src.models.text_encoder.MVLMTextEncoder
    name: 'emilyalsentzer/Bio_ClinicalBERT'
    num_layers_to_freeze: 0
  itc_head:
    _target_: src.models.projection.ITCHead
    img_hidden_size: 768
    text_hidden_size: 768
    projection_size: ${proj_dim_size}
  itm_head:
    _target_: src.models.projection.ITMHead
    hidden_size: 768 # only works if text and image model have same hidden size
  img_cross_encoder:
    _target_: src.models.components.cross_modality.ImgCrossModalityEncoder
    hidden_dim: 768
    num_heads: 4 # TODO no information in the paper
    num_layers: 3
  txt_cross_encoder:
    _target_: src.models.components.cross_modality.TxtCrossModalityEncoder
    hidden_dim: 768
    num_heads: 4 # TODO no information in the paper
    num_layers: 3
  img_cross_decoder:
    _target_: src.models.components.cross_modality.ImgCrossModalityDecoder
    hidden_dim: 768
    num_heads: 4 # TODO no information in the paper
    num_layers: 3
    patch_size: ${model.image_patch_size}
    channels: 3
  txt_classifier:
    _target_: src.models.components.txt_decoder.TextTokenClassifier
    hidden_dim: 768
    vocab_size: 30522 # TODO: Check if this is correct
