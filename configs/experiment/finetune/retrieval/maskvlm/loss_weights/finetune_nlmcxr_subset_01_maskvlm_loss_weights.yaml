# @package _global_

# to execute this experiment run:
# python train.py experiment=example

# defaults copied from pretraining.yaml and overridden here
defaults:
  - override /datamodule: nlmcxr.yaml
  - override /model: nlmcxr_finetune_maskvlm_model.yaml
  - override /callbacks: nlmcxr_finetune_callbacks.yaml
  - override /logger: wandb
  - override /trainer: nlmcxr_finetune_trainer.yaml
  - override /paths: nlmcxr_finetune_paths.yaml
  - override /hydra: default.yaml
  - override /extras: default.yaml
  - override /debug: null

task_name: "nlmcxr_subset_01_finetune_maskvlm_loss_weights"
train: True
test: False
seed: 42

model:
  model:
    checkpoint_path: '/vol/miltank/projects/practical_WS2425/vision_language/models/maskvlm_loss_weights/checkpoints/epoch_058.ckpt'

datamodule: 
  subset_percentage: 0.01 # 1% of the data

logger:
  wandb:
    project: "finetuning_retrieval"
    entity: "adlm-vision-language"
    name: "nlmcxr_subset_01_finetune_maskvlm_loss_weights"
    tags:
      - "finetuning"
      - "nlmcxr"
      - "subset_01"
      - "maskvlm_loss_weights"
      # - "test"
    notes: "MaskVLM Loss Weights NLMCXR finetuning with default configuration"

# trainer:
#   max_epochs: 10
#   val_check_interval: 1000