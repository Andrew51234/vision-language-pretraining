# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: mimic_cxr_mvlm.yaml
  - override /model: mvlm_model.yaml
  - override /callbacks: maskvlm_pretrain_callbacks.yaml
  - override /trainer: maskvlm_pretrain_trainer.yaml
  - override /paths: default.yaml

task_name: "maskvlm_freeze_bert"
train: True
test: False
seed: 42

logger:
  wandb:
    project: "pretraining"
    entity: "adlm-vision-language"
    name: ${task_name}
    tags:
      - "pretraining"
      - "mimic_cxr"
      - "freeze_bert"
      # - "test"
      - "maskvlm"
    notes: "Maskvlm pretraining with some layers of BERT frozen"

model:
  net:
    text_model:
      num_layers_to_freeze: 6 # convirt freezes the first 6 layers of BERT

